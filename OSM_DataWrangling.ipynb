{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Street Map Project - Austin, TX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, I worked with the Open Street Maps project data for city of Austin. The datafile was approximately 800 MB.\n",
    "I started by loading the Python library for parsing XML files incrementally (cElementTree). I also included the Geocoder library as I used to calculate zipcode from latitude and longitude information available for nodes with certain specific features that interested me. In addition, I imported some more library to help with several other tasks as part of the data wrangling process including regular expressions, json (for file writing), pretty print (to print results that can be read easily), string (for string manipulation), ast (to convert a string variable into a dictionary during the file read process), and pymongo for communicating with mongodb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from geopy.geocoders import Nominatim\n",
    "#from googlemaps import GoogleMaps\n",
    "import time\n",
    "import codecs\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import pprint\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import ast\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined the regular expressions and three list variables in the next section. I use these list variables to narrow down the list of features I want to focus for the node and way tags. In addition, my initial analysis suggested that while we are focusing on OSM data from Austin, many entries included the names for respective sub-divisions within Austin and they are all valid entries as many people refer by those names. As this information provided more detail on the locale for the respective entries, I chose to include te range of values within a defined list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zipcode_re = re.compile(r'^[0-9]{5}')\n",
    "city_comma_re = re.compile(r'^[A-Za-z ]+,')\n",
    "count = 0\n",
    "street_types = {}\n",
    "\n",
    "list_of_cities = ['Austin','Cedar Park','Round Rock','West Lake Hills','Pflugerville','Sunset Valley','Del Valle']\n",
    "node_tags_to_save = [\"amenity\",\"cuisine\",\"public_transport\",\"highway\",\"shelter\",\"shop\",\"golf\",\"emergency\",\"name\",\\\n",
    "               \"bicycle_parking\",\"man_made\",\"sport\",\"religion\",\"website\",\"building\"]\n",
    "way_tags_to_save = [\"amenity\",\"bicycle\",\"bridge\",\"golf\",\"height\",\"highway\",\"name\",\"landuse\"]\n",
    "zipcode_to_calc = [\"amenity\",\"cuisine\",\"public_transport\",\"highway\",\"shelter\",\"shop\",\"golf\",\"bicycle_parking\",\\\n",
    "                       \"sport\",\"religion\",\"building\"]\n",
    "node_tags = ['id']\n",
    "creation_tags = ['changeset', 'uid', 'timestamp', 'version', 'user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, just as in the exercises in class, I started with the fields which I expected to have an order - city, zipcode, county, etc. In my case I focused on two of the address fields - city and zipcode. The city field values contained two kinds of anomalies. The first was the city was suffixed by the abbreviation for the state (eg., Austin, TX) and the second group included address values that were suffixed by the city (eg., 614 W. 6th Street, Austin). The name of a city typically does not contain any number and therefore a regular expression that includes only letters in upper or lower case and spaces (city names can be two words or more) will help segregate the anomalies as they contain street numbers or commas. For the first group, the portion of the string before the first comma was used as the city value. For the second category, the portion of the string after the last comma was used as the city value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_city(city_val):\n",
    "    if city_val in list_of_cities or city_val.title() in list_of_cities:\n",
    "        city = city_val.title()\n",
    "    elif city_comma_re.search(city_val):\n",
    "        city = city_val[:string.find(city_val,\",\")]\n",
    "    elif city_val[string.find(city_val,\",\")+2:] in list_of_cities:\n",
    "        city = tag.attrib[\"v\"][string.find(city_val,\",\")+2:]\n",
    "    if city_val.endswith(\", TX\") or city_val.endswith(\", tx\"):\n",
    "        state = \"TX\"\n",
    "    if state != None:\n",
    "        return (city,state)\n",
    "    else:\n",
    "        return (city,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zipcode values also included two kinds of anomalies. The first group included entries where the five digit zipcode was suffixed with the four digit PO box separated by a dash(-). The second group included records that were prefixedwith \"TX, \". I wrote a regular expression where the target value must start with five consecutive numbers. This would include proper five digit zipcode entries as well as the anomalies from the first group. So I obtained a substring comprising of the first five characters for this group which would return the five-digit zipcode, whether it is an anomaly or not. For the second category, I took a similar approach but this time I obtined a substring comprising of the fourth through the eigth character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_zip(zip_val):\n",
    "    if zipcode_re.search(zip_val):\n",
    "        zip = zip_val[0:5]\n",
    "    elif tag.attrib[\"v\"].startswith(\"TX \"):\n",
    "        zip = zip_val[3:8]\n",
    "        state = \"TX\"\n",
    "    if state != None:\n",
    "        return (zip,state)\n",
    "    else:\n",
    "        return (zip,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the housenumber, the street categories though highly variable looked reasonable. Unfortunately, given the variety in the data, I considered this data to be extremely difficult to validate against any master data set or apply any kind of generalization rules. So I consumed this data as is.\n",
    "Most of the header information are similar between the node and way tags except for the latitude and longitude, which is only available for the nodes. The location information was available for a significant number of the node entries and I tried to calculate the zip code for the ones that lacked address information. While I was partially successful, I faced challenges when calculating the zip code in bulk as the remote server caps the number of successive API calls within an interval of time and when violated issues \"HTTP 429: too many requests\" exception (http://stackoverflow.com/questions/22786068/how-to-avoid-http-error-429-too-many-requests-python).\n",
    "The location information associated with the node tags made me more interested in the features for the nodes dataset which is evident from the defined lists in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reshape_tag(elem):\n",
    "    \n",
    "    node_ref = []\n",
    "    node_info = {}\n",
    "    creation_info = {}\n",
    "    address_info = {}\n",
    "    \n",
    "    for record in elem.attrib:\n",
    "        if record in node_tags:\n",
    "            node_info[record] = elem.attrib[record]\n",
    "        elif record in creation_tags:\n",
    "            creation_info[record] = elem.attrib[record]\n",
    "    for tag in elem.iter(\"tag\"):\n",
    "        if tag.attrib[\"k\"].startswith(\"addr:city\"):\n",
    "            address_info[\"city\"] = format_city(tag.attrib[\"v\"])[0]\n",
    "            if len(format_city(tag.attrib[\"v\"])) > 1: address_info[\"state\"] = format_city(tag.attrib[\"v\"])[1]\n",
    "        elif tag.attrib[\"k\"].startswith(\"addr:postcode\"):\n",
    "            address_info[\"zipcode\"] = format_zip(tag.attrib[\"v\"])[0]\n",
    "            if len(format_zip(tag.attrib[\"v\"])) > 1: address_info[\"state\"] = format_zip(tag.attrib[\"v\"])[1]\n",
    "        elif tag.attrib[\"k\"].startswith(\"addr:state\"):\n",
    "            address_info[\"state\"] = tag.attrib[\"v\"]\n",
    "        elif tag.attrib[\"k\"].startswith(\"addr:street\"):\n",
    "            address_info[\"street\"] = tag.attrib[\"v\"]\n",
    "        elif elem.tag == \"node\" and tag.attrib[\"k\"] in node_tags_to_save:\n",
    "            if tag.attrib[\"k\"] == \"cuisine\": node_info['cuisine'] = tag.attrib[\"v\"].split(\";\")\n",
    "            else: node_info[tag.attrib[\"k\"]] = tag.attrib[\"v\"]\n",
    "        elif elem.tag == \"way\" and tag.attrib[\"k\"] in way_tags_to_save:\n",
    "            node_info[tag.attrib[\"k\"]] = tag.attrib[\"v\"]\n",
    "            \n",
    "    for tag in elem.iter(\"nd\"):\n",
    "        node_ref.append(tag.attrib[\"ref\"])\n",
    "            \n",
    "    node_info['created'] = creation_info.copy()\n",
    "    if elem.tag == \"node\": node_info['location'] = [float(elem.attrib[\"lat\"]),float(elem.attrib[\"lon\"])]\n",
    "\n",
    "    for tag in zipcode_to_calc:\n",
    "        if tag in node_info:\n",
    "            if \"zipcode\" not in address_info:\n",
    "                try:\n",
    "                    geolocator = Nominatim()\n",
    "                    location = geolocator.reverse(elem.attrib[\"lat\"]+\",\"+elem.attrib[\"lon\"], timeout=10000)\n",
    "                    #time.sleep(0.5)\n",
    "                    address_info[\"zipcode\"] = location.raw[\"address\"][\"postcode\"]\n",
    "                except: pass\n",
    "    if address_info != {}: node_info['address'] = address_info.copy()\n",
    "        \n",
    "    if elem.tag == \"way\": node_info[\"node_ref\"] = node_ref[:]\n",
    "        \n",
    "    return node_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I process the file downloaded using the Overpass API. I accumulate the node entries in the \"OSM_data_nodes.json\" and the way entries in \"OSM_data_ways.json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILENAME = \"map\"\n",
    "\n",
    "all_tags = {}\n",
    "\n",
    "all_nodes = []\n",
    "all_ways = []\n",
    "\n",
    "file_out_nodes = \"OSM_data_nodes.json\"\n",
    "file_out_ways = \"OSM_data_ways.json\"\n",
    "\n",
    "with open(FILENAME,'r') as fl:\n",
    "    for event, elem in ET.iterparse(fl):\n",
    "        if elem.tag not in all_tags:\n",
    "            all_tags[elem.tag] = 1\n",
    "        else:\n",
    "            all_tags[elem.tag] += 1\n",
    "        if elem.tag == \"node\":\n",
    "            all_nodes.append(reshape_tag(elem))\n",
    "        elif elem.tag == \"way\":\n",
    "            all_ways.append(reshape_tag(elem))\n",
    "                    \n",
    "\n",
    "\n",
    "with codecs.open(file_out_nodes, \"w\") as fon:\n",
    "    for record in all_nodes:\n",
    "        fon.write(json.dumps(record) + \"\\n\")\n",
    "            \n",
    "with codecs.open(file_out_ways, \"w\") as fow:\n",
    "    for record in all_ways:\n",
    "        fow.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gross Statistics about the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file size for the \"nodes\" and \"ways\" dataset is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes file size: 630034550 Bytes\n",
      "Ways file size: 120743393 Bytes\n"
     ]
    }
   ],
   "source": [
    "print \"Nodes file size: \" + str(os.path.getsize(\"OSM_data_nodes.json\")) + \" Bytes\"\n",
    "print \"Ways file size: \" + str(os.path.getsize(\"OSM_data_ways.json\")) + \" Bytes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I establish the DB connection here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost:27017')\n",
    "db = client.OSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I insert all the node records into the \"nodes\" collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for record in all_nodes:\n",
    "    db.nodes.insert_one(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I insert all the way records into the \"ways\" collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for record in all_ways:\n",
    "    db.ways.insert_one(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes dataset contain the location details for each and every node, so I added a spatial index so that I can issue spatial queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ambarishbanerjee/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: DeprecationWarning: ensure_index is deprecated. Use create_index instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'location_2d'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.nodes.ensure_index([('location',pymongo.GEO2D)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started by querying the \"nodes\" collection to see how many nodes were in the dataset. The result shows our dataset contained approximately 3.15 million nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'count': 3150468}]\n"
     ]
    }
   ],
   "source": [
    "pipeline_all_node_count = [{\"$group\":{\"_id\":\"All Nodes\",\"count\":{\"$sum\":1}}},{\"$project\":{\"_id\":0,\"count\":1}}]\n",
    "result_all_node_count = [doc for doc in db.nodes.aggregate(pipeline_all_node_count)]\n",
    "pprint.pprint(result_all_node_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I did the same with the \"ways\" collection and found that there were approximately 340,000 ways in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'count': 339908}]\n"
     ]
    }
   ],
   "source": [
    "pipeline_all_way_count = [{\"$group\":{\"_id\":\"All Ways\",\"count\":{\"$sum\":1}}},{\"$project\":{\"_id\":0,\"count\":1}}]\n",
    "result_all_way_count = [doc for doc in db.ways.aggregate(pipeline_all_way_count)]\n",
    "pprint.pprint(result_all_way_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I was curious to see the top three contributors for the \"nodes\" dataset and determine if the list was the same for the ways dataset. I noticed that the lists were the same with identical ordering in the top three ranks for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'patisilva_atxbuildings', u'count': 1472202},\n",
      " {u'_id': u'ccjjmartin_atxbuildings', u'count': 433859},\n",
      " {u'_id': u'ccjjmartin__atxbuildings', u'count': 246389}]\n",
      "\n",
      "[{u'_id': u'patisilva_atxbuildings', u'count': 161307},\n",
      " {u'_id': u'ccjjmartin_atxbuildings', u'count': 39961},\n",
      " {u'_id': u'ccjjmartin__atxbuildings', u'count': 24059}]\n"
     ]
    }
   ],
   "source": [
    "pipeline_highest_contrib = [{\"$group\":{\"_id\":\"$created.user\",\"count\":{\"$sum\":1}}},{\"$sort\":{\"count\":-1}}]\n",
    "result_highest_contrib = [doc for doc in db.nodes.aggregate(pipeline_highest_contrib)]\n",
    "pprint.pprint(result_highest_contrib[:3])\n",
    "print \"\"\n",
    "\n",
    "pipeline_highest_contrib_way = [{\"$group\":{\"_id\":\"$created.user\",\"count\":{\"$sum\":1}}},{\"$sort\":{\"count\":-1}}]\n",
    "result_highest_contrib_way = [doc for doc in db.ways.aggregate(pipeline_highest_contrib_way)]\n",
    "pprint.pprint(result_highest_contrib_way[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I wanted to see how many unique contributors we had for the \"nodes\" and \"ways\" datasets/collections. There were 704 unique contributors for the \"nodes\" collection and 529 for the \"ways\" collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'count': 704}]\n",
      "\n",
      "[{u'count': 529}]\n"
     ]
    }
   ],
   "source": [
    "pipeline_unique_users = [{\"$group\":{\"_id\":\"Unique Users\",\"unique_users\":{\"$addToSet\":\"$created.user\"}}},\\\n",
    "                        {\"$unwind\":\"$unique_users\"},{\"$group\":{\"_id\":\"Total Users\",\"count\":{\"$sum\":1}}},\\\n",
    "                         {\"$project\":{\"_id\":0,\"count\":1}}]\n",
    "result_unique_users = [doc for doc in db.nodes.aggregate(pipeline_unique_users)]\n",
    "pprint.pprint(result_unique_users)\n",
    "print \"\"\n",
    "\n",
    "pipeline_unique_users_ways = [{\"$group\":{\"_id\":\"Unique Users\",\"unique_users\":{\"$addToSet\":\"$created.user\"}}},\\\n",
    "                        {\"$unwind\":\"$unique_users\"},{\"$group\":{\"_id\":\"Total Users\",\"count\":{\"$sum\":1}}},\\\n",
    "                         {\"$project\":{\"_id\":0,\"count\":1}}]\n",
    "result_unique_users_ways = [doc for doc in db.ways.aggregate(pipeline_unique_users_ways)]\n",
    "pprint.pprint(result_unique_users_ways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Few Interesting Highlights about the Austin OSM Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this query, I wanted to determine the top five zipcodes that had the highest concentration of restaurants. 78704 ranked highest on the list. \n",
    "PS > There were several instances for which the address fields were unknown and hence these were filtered from our recordset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'78704', u'count': 16},\n",
      " {u'_id': u'78757', u'count': 11},\n",
      " {u'_id': u'78741', u'count': 9},\n",
      " {u'_id': u'78702', u'count': 8},\n",
      " {u'_id': u'78705', u'count': 7}]\n"
     ]
    }
   ],
   "source": [
    "pipeline_zipcode_restaurant = [{\"$match\":{\"cuisine\":{\"$exists\":1}}},{\"$match\":{\"address.zipcode\":{\"$exists\":1}}},\\\n",
    "                               {\"$group\":{\"_id\":\"$address.zipcode\",\"count\":{\"$sum\":1}}},{\"$sort\":{\"count\":-1}}]\n",
    "result = [doc for doc in db.nodes.aggregate(pipeline_zipcode_restaurant)]\n",
    "pprint.pprint(result[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this query, I tried to obtain the five most popular cusine in Austin. Interestingly, \"Mexican\" ranked highest in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'mexican', u'count': 62},\n",
      " {u'_id': u'sandwich', u'count': 34},\n",
      " {u'_id': u'burger', u'count': 32},\n",
      " {u'_id': u'pizza', u'count': 28},\n",
      " {u'_id': u'coffee_shop', u'count': 19}]\n"
     ]
    }
   ],
   "source": [
    "pipeline_popular_cuisine = [{\"$match\":{\"cuisine\":{\"$exists\":1}}},\\\n",
    "                            {\"$unwind\":\"$cuisine\"},{\"$group\":{\"_id\":\"$cuisine\",\"count\":{\"$sum\":1}}},{\"$sort\":{\"count\":-1}}]\n",
    "result_pop_cuisine = [doc for doc in db.nodes.aggregate(pipeline_popular_cuisine)]\n",
    "pprint.pprint(result_pop_cuisine[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I wanted to look at some descriptive statistics for the \"ways\" that had references to nodes. While the highest number of nodes referenced in a single \"way\" tag was ~1800, the average was about 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'33171423', u'count': 1806}]\n",
      "\n",
      "[{u'_id': u'Average Node Count', u'avgCount': 10.230924250091201}]\n"
     ]
    }
   ],
   "source": [
    "max_node_way_pipeline = [{\"$unwind\":\"$node_ref\"},{\"$group\":{\"_id\":\"$id\",\"count\":{\"$sum\":1}}},{\"$sort\":{\"count\":-1}},\\\n",
    "                         {\"$limit\":1}]\n",
    "result_max_node_way = [doc for doc in db.ways.aggregate(max_node_way_pipeline)]\n",
    "pprint.pprint(result_max_node_way)\n",
    "print \"\"\n",
    "\n",
    "avg_node_way_pipeline = [{\"$unwind\":\"$node_ref\"},{\"$group\":{\"_id\":\"$id\",\"count\":{\"$sum\":1}}},\\\n",
    "                         {\"$group\":{\"_id\":\"Average Node Count\",\"avgCount\":{\"$avg\":\"$count\"}}}]\n",
    "result_avg_node_way = [doc for doc in db.ways.aggregate(avg_node_way_pipeline)]\n",
    "pprint.pprint(result_avg_node_way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to locate the five closest restaurants by the University of Texas. For this, I used the spatial index that was created earlier on the \"location\" field. Interestingly, the top restaurant's cusine includes \"\\*\" implying everything. \n",
    "Side note: I know from experience that thisis true as this establishment is part of one of the dormitories that has a food court which serves just about everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'cuisine': [u'*'], u'name': u'Dobie Tower Food Court'},\n",
      " {u'cuisine': [u'asian'], u'name': u\"Emo's Kitchen\"},\n",
      " {u'cuisine': [u'sandwich'], u'name': u'Bite Mi'},\n",
      " {u'cuisine': [u'thai'], u'name': u'Thai How Are You'},\n",
      " {u'cuisine': [u'indian'], u'name': u\"Teji's\"}]\n"
     ]
    }
   ],
   "source": [
    "university_location = {\"amenity\":{\"$exists\":1},\"amenity\":\"university\"}\n",
    "university_loc_proj = {\"_id\":0,\"location\":1}\n",
    "result_univ_loc = [doc for doc in db.nodes.find(university_location,university_loc_proj)]\n",
    "univ_loc = result_univ_loc[:1][0][\"location\"]\n",
    "\n",
    "\n",
    "query_location_based = {\"location\":{\"$near\":univ_loc},\"cuisine\":{\"$exists\":1}}\n",
    "project_location_based = {\"_id\":0,\"name\":1,\"cuisine\":2}\n",
    "result_location_based = [doc for doc in db.nodes.find(query_location_based,project_location_based)]\n",
    "pprint.pprint(result_location_based[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested Improvements and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing I felt could have added a lot of value to the OSM data is the availability of zipcode or some measure of locale that is easily relatable. The information provided as part of the OSM data was most likely obtained using mobile devices. Most devices that allow geotagging can determine the zipcode and rough approximations for the street address. Users who are providing the information can greatly help the consumers of this information by providing some geographic measure of the locale along with GPS coordinates. This would facilitate the grouping of information by location in a more logical sense. Possible benefits of this enhancement can include, zip code with the highest number of schools or parks or any other amenity of choice to the user. \n",
    "I attempted to calclulate the same but restrictions enforced on the server hosting the API prevented me from doing so. So I thought if this information was available, it would have added considerable value to the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
